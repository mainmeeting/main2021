[
{
    "name" : "Arvind Kumar",
    "last_name": "Kumar",
    "affiliation": "KTH Royal Institute of Technology, Stockholm",
    "title" : "Introduction to Neuroscience",
    "url" : "https://www.kth.se/profile/arvindku",
    "image_src": "assets/img/arvindku.jpeg",
    "field": "Neuroscience",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "0900",
     "abstract": "I will give an overview of neuroscience so that the audience new to this field can learn about the key concepts and main terminology necessary to follow the discussions in the meeting."
},
{
    "name" : "Srikanth Ramaswamy",
    "last_name": "Ramaswamy",
    "affiliation": "KTH Royal Institute of Technology, Stockholm",
    "title" : "Introduction to Neuroscience",
    "url" : "https://people.epfl.ch/srikanth.ramaswamy",
    "image_src": "assets/img/Srikanth_Ramaswamy.jpeg",
    "field": "Neuroscience",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "0930",
     "abstract": ""
},
{
    "name" : "Saikat  Chatterjee",
    "last_name": "Chatterjee",
    "affiliation": "KTH Royal Institute of Technology, Stockholm",
    "title" : "Introduction to AI",
    "url" : "https://www.kth.se/profile/sach",
    "image_src": "assets/img/Saikat_Chatterjee.png",
    "field": "AI",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1000",
     "abstract": ""
},
{
    "name" : "Martina Scolamiero",
    "last_name": "Scolamiero",
    "affiliation": "KTH Royal Institute of Technology, Stockholm",
    "title" : "TBA",
    "url" : "https://www.kth.se/profile/scola",
    "image_src": "assets/img/MartinaScolamiero.jpeg",
    "field": "Topology",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1300",
     "abstract": ""
},
{
    "name" : "Wojciech Chachólski",
    "last_name": "Chachólski",
    "affiliation": "KTH Royal Institute of Technology, Stockholm",
    "title" : "TBA",
    "url" : "https://www.kth.se/profile/wojtek",
    "image_src": "assets/img/wojciech_chacholski.jpeg",
    "field": "Topology",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1000",
     "abstract": ""
},    
{
    "name" : "Ran Levi",
    "last_name": "Levi",
    "affiliation": "University of Aberdeen, Scotland, UK",
    "title" : "TBA",
    "url" : "https://www.abdn.ac.uk/ncs/profiles/r.levi/",
    "image_src": "assets/img/Ran_Levi.jpeg",
    "field": "Topology",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1340",
     "abstract": ""
},
{
    "name" : "Antonio Marquez",
    "last_name": "Marquez",
    "affiliation": "King Juan Carlos University Camino del Molino, Madrid, Spain",
    "title" : "Connecting the dots: Identifying Network Structure using Graph Signal Processing",
    "url" : "https://tsc.urjc.es/~amarques/",
    "image_src": "assets/img/antonio_marquez.jpeg",
    "field": "AI",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1420",
     "abstract": "The talk provides an overview of graph signal processing (GSP)-based methods designed to learn an unknown network from nodal observations. Using signals to learn a graph is a central problem in network science and statistics, with results going back more than 50 years. The main goal of the talk is threefold: i) explaining in detail fundamental GSP-based methods and comparing those with classical methods in statistics, ii) putting forth a number of GSP-based formulations and algorithms able to address scenarios with a range of different operating conditions, and iii) briefly introducing generalizations to more challenging setups, including multi-layer graphs and learning in the presence of hidden nodal variables. Our graph learning algorithms will be designed as solutions to judiciously formulated constrained-optimization sparse-recovery problems. Critical to this approach is the codification of GSP concepts such as signal smoothness and graph stationarity into tractable constraints. "
},
{
    "name" : "Carina Curto",
    "last_name": "Curto",
    "affiliation": "The Pennsylvania State University",
    "title" : "TBA",
    "url" : "http://www.personal.psu.edu/cpc16/",
    "image_src": "assets/img/carina_curto.jpeg",
    "field": "Maths",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1600",
     "abstract": ""
},
{
    "name" : "Daniela Egas Santander",
    "last_name": "Santander",
    "affiliation": "EPFL, Switzerland",
    "title" : "Nerve theorems for fixed points of neural networks",
    "url" : "https://opc.mfo.de/detail?photo_id=19248",
    "image_src": "assets/img/Daniela_Egas.jpeg",
    "field": "Topology",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1640",
     "abstract": ""
},
{
    "name" : "Vladimir Itskov",
    "last_name": "Itskov",
    "affiliation": "The Pennsylvania State University, PA, US",
    "title" : "Decoding geometry and topology of neural representations",
    "url" : "http://personal.psu.edu/vui1/",
    "image_src": "assets/img/vladimir_itskov.jpeg",
    "field": "Topology",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1720",
    "abstract": "The brain represents the perceived world via the activity of individual neurons. Neural activity in many systems is organized on low-dimensional manifolds. Understanding the neural representations (a.k.a. the neural code) thus requires methods for inferring the structure of the underlying stimulus space, as well as natural decoding mechanisms that takes advantage of this structure. Neural representations are constrained by receptive field properties of individual neurons as well as the underlying neural network. It is therefore essential to utilize these constraints in any meaningful analysis of the underlying space. In my talk, I will describe two different methods, based on computational topology and differential geometry that take advantage of the receptive field properties to infer the dimension of (non-linear) neural representations as well as a geometry-based learning algorithm, that can be re-interpreted as an output of a neural network.  I will illustrate the first method by inferring basic features of the neural representations in the mouse olfactory bulb."
},
{
    "name" : "Damian Battaglia",
    "last_name": "Battaglia",
    "affiliation": "University of Strasbourg, France",
    "title" : "Neuronal dynamics and dynamics of information",
    "url" : "https://www.usias.fr/en/fellows/fellows-2020/demian-battaglia/",
    "image_src": "assets/img/Damian_Battaglia.jpg",
    "field": "Neuroscience",
    "date": "13.12.2021",
    "day": "Monday",
    "time": "1800",
     "abstract": "Theta and gamma oscillations are believed to organise hippocampal activity and function. The current dominant view posits the existence of two gamma frequency sub-bands, occurring at different theta phases in CA1, produced by different generators and sub-serving exclusive cognitive operations. Such views are however grounded on averaging over many oscillatory events. Here, we explore agnostically the diversity of individual theta-gamma bursts and find, in striking contrast with the discrete sub-bands hypothesis, that gamma bursts with nearly every combination of frequency, amplitude and theta-phase can occur in every CA1 layer. Average oscillatory spectra reflect indeed only a minority of strong power events, overshadowing pervasive diversity. In other words, averages don’t exist. Is this oscillatory diversity to be expected or not? To answer this first question, we constructed a spiking computational model of local recurrent circuitry, involving one excitatory and one inhibitory (fast-spiking-like) population. We reveal then that, for most parameter combinations, complex oscillations fluctuating over the entire gamma spectrum arise, as observed in data, without need for any fine tuning. Second, is this diversity functional or not? To probe whether theta-gamma variability reflects noise or, on the contrary, extractable information about behavior, we used a machine learning approach to decode from individual theta-gamma oscillatory bursts the running speed and the coarse position of mice navigating in a maze to seek for reward. We found that behavioral features can be decoded with large, above chance-level accuracy in all probed hippocampal layers. Furthermore, different “styles of complexity” are observed for different layers, as well as for different stages of task learning. Altogether, our findings suggest that hippocampal oscillatory diversity is not mere noise but carries an actual encoding of context and behavior, complex in nature and not reducible to simpler descriptions in terms of a few reference bands."
},
{
    "name" : "Lars Kai Hansen",
    "last_name": "Hansen",
    "affiliation": "Technical University of Denmark, Denmark",
    "title" : "Machine learning for EEG based brain state monitoring",
    "url" : "http://cogsys.imm.dtu.dk/staff/lkhansen/lkhansen.html",
    "image_src": "assets/img/lars_kai_hansen.jpeg",
    "field": "AI",
    "date": "14.12.2021",
    "day": "Tuesday",
    "time": "1300",
    "abstract": "Electroencephalography (EEG) is a widely used non-invasive technology for brain monitoring. The EEG signal is complex and confounded by artifacts and EEG analysis is challenging. I will review progress in machine learning for EEG analysis and discuss our efforts to push the limits to EEG based brain state monitoring."
},
{
    "name" : "Partha Mitra",
    "last_name": "Mitra",
    "affiliation": "Cold Spring Harbor Laboratory, US",
    "title" : "Fitting elephants in modern Machine Learning: Interpolating noisy data and getting away with it",
    "url" : "http://brainarchitecture.org/",
    "image_src": "assets/img/Partha_Mitra.jpeg",
    "field": "Neuroscience",
    "date": "14.12.2021",
    "day": "Tuesday",
    "time": "1340",
     "abstract": "The standard practice in machine learning and statistical regression is to optimize an empirical loss function that quantifies the goodness-of-fit to a training data set, explicitly penalized by a cost function that controls model complexity, or implicitly through the optimization dynamics. However, in the big data limit, overparameterized artificial neural network models seem to generalize well even when interpolating the training data. This shows that a radically simpler approach to supervised learning may be possible – namely one could simply interpolate the training data points without parametric optimization of loss functions. This may be anathema to textbook statistics but seems to be the regime that big-data machine learning operates in. While there is now a rapidly growing literature studying overparameterized modes in the interpolating regime, there is comparatively little direct study of data interpolation per se as a supervised learning technique. Scattered data interpolation is a well-developed subject, for example in computer graphics, but is typically not applied to noisy data, with the rationale that such data requires regularization or smoothing. Nevertheless, we now understand that noisy data interpolation can in fact produce statistically optimal function fits, in the sense of asymptotic consistency in the large sample limit (Statistically Consistent Interpolation). Such schemes are theoretically attractive due to transparency and tractability, as they are white-box techniques without hidden structure and directly reflect the training data set. In this talk, we discuss an interesting universal interpolation scheme belonging to the Nadaraya-Watson family of estimators, which is unique in the sense that it does not have any tunable parameters (including any bandwidth selection parameters) but exhibits statistical consistency in the large sample limit under broad conditions. Statistical consistency was previously proven for this so-called Hilbert kernel interpolation scheme, but convergence rates were not computed. We were able to explicitly compute the convergence rates for both regression and classification, including the exact pre-factors to the leading terms. The convergence is found to be logarithmic in sample size, and interestingly, is dominated by the variance term under generic circumstances, so that there is no bias-variance tradeoff. We observe a universal power-law behavior of the data weights in the large sample size limit and prove several theorems that comprehensively characterize the large (but finite) sample behavior of this estimator."
},
{
    "name" : "Mackenzie Mathis",
    "last_name": "Mathis",
    "affiliation": "EPFL, Switzerland",
    "title" : "Understand the brain through the lens of behavior",
    "url" : "http://www.mackenziemathislab.org/",
    "image_src": "assets/img/Mackenzie_Mathis.jpeg",
    "field": "Neurosciece, AI",
    "date": "14.12.2021",
    "day": "Tuesday",
    "time": "1420",
     "abstract": "I will discuss our latest research program merging neuroscience and computer vision for the study of the brain. In particular, I will discuss DeepLabCut, our tool for animal pose estimation. "
},
{
    "name" : "Yi Ma",
    "last_name": "Ma",
    "affiliation": "University of California, Berkeley, CL, UA",
    "title" : "Closed-Loop Data Transcription via Minimaxing Rate Reduction",
    "url" : "https://people.eecs.berkeley.edu/~yima/",
    "image_src": "assets/img/YiMa.jpeg",
    "field": "AI",
    "date": "14.12.2021",
    "day": "Tuesday",
    "time": "1600",
     "abstract": "This work proposes a new computational framework for learning an explicit generative model for real-world datasets. More specifically, we propose to learn a closed-loop transcription between a multi-class multi-dimensional data distribution and a linear discriminative representation (LDR) in the feature space that consists of multiple independent linear subspaces. We argue that the optimal encoding and decoding mappings sought can be formulated as the equilibrium point of a two-player minimax game between the encoder and decoder. A natural utility function for this game is the so-called rate reduction, a simple information-theoretic measure for distances between mixtures of subspace-like Gaussians in the feature space. Our formulation draws inspiration from closed-loop error feedback from control systems and avoids expensive evaluating and minimizing approximated distances between arbitrary distributions in either the data space or the feature space. To a large extent, this new formulation unifies the concepts and benefits of Auto-Encoding and GAN and naturally extends them to the settings of learning a both discriminative and generative representation for multi-class and multi-dimensional real-world data. Our extensive experiments on many benchmark imagery datasets demonstrate tremendous potential of this new closed-loop formulation: we notice that the so learned features of different classes are explicitly mapped onto approximately independent principal subspaces in the feature space; and diverse visual attributes within each class are modeled by the independent principal components within each subspace. This work opens many deep mathematical problems regarding learning submanifolds in high-dimensional spaces as well as suggests potential computational mechanisms about how memory can be formed through a purely internal close-loop process."
},
{
    "name" : "Yonina Eldar",
    "last_name": "Eldar",
    "affiliation": "University of California, Berkeley, CL, UA",
    "title" : "Model Based Deep Learning: Applications to Imaging and Communications",
    "url" : "https://www.wisdom.weizmann.ac.il/~yonina/YoninaEldar/",
    "image_src": "assets/img/yonina.jpeg",
    "field": "AI",
    "date": "14.12.2021",
    "day": "Tuesday",
    "time": "1640",
     "abstract": "Deep neural networks provide unprecedented performance gains in many real-world problems in signal and image processing. Despite these gains, the future development and practical deployment of deep networks are hindered by their black-box nature, i.e., a lack of interpretability and the need for very large training sets. On the other hand, signal processing and communications have traditionally relied on classical statistical modeling techniques that utilize mathematical formulations representing the underlying physics, prior information and additional domain knowledge. Simple classical models are useful but sensitive to inaccuracies and may lead to poor performance when real systems display complex or dynamic behavior. Here we introduce various approaches to model based learning which merge parametric models with optimization tools leading to efficient, interpretable networks from reasonably sized training sets. We will consider examples of such model-based deep networks to image deblurring, image separation, super resolution in ultrasound and microscopy, efficient communications systems, and finally we will see how model-based methods can also be used for efficient diagnosis of COVID19 using X-ray and ultrasound."
},
{
    "name" : "Michael Felsberg",
    "last_name": "Felsberg",
    "affiliation": "Linköpings universitet, Sweden",
    "title" : "TBA",
    "url" : "https://liu.se/medarbetare/micfe03",
    "image_src": "assets/img/Felsberg.jpeg",
    "field": "AI",
    "date": "14.12.2021",
    "day": "Tuesday",
    "time": "1720",
     "abstract": "TBA"
},
{
    "name" : "Michael London",
    "last_name": "London",
    "affiliation": "The Hebrew University, Jerusalem, Israel",
    "title" : "Single cortical neuron as a deep artificial neural network",
    "url" : "https://scholars.huji.ac.il/jbc/people/dr-mickey-london",
    "image_src": "assets/img/mickey_london.png",
    "field": "Neuroscience",
    "date": "15.12.2021",
    "day": "Wednesday",
    "time": "1300",
     "abstract": "Utilizing recent advances in machine learning, we introduce a systematic approach to characterize neurons’ input/output (I/O) mapping complexity. Deep neural networks (DNNs) were trained to faithfully replicate the I/O function of various biophysical models of cortical neurons at millisecond (spiking) resolution. A temporally convolutional DNN with five to eight layers was required to capture the I/O mapping of a realistic model of a layer 5 cortical pyramidal cell (L5PC). This DNN generalized well when presented with inputs widely outside the training distribution. When NMDA receptors were removed, a much simpler network (fully connected neural network with one hidden layer) was sufficient to fit the model. Analysis of the DNNs’ weight matrices revealed that synaptic integration in dendritic branches could be conceptualized as pattern matching from a set of spatiotemporal templates. This study provides a unified characterization of the computational complexity of single neurons and suggests that cortical networks therefore have a unique architecture, potentially supporting their computational power."
},
{
    "name" : "Tim Vogels",
    "last_name": "Tim",
    "affiliation": "Institute of Science and Technology Austria (IST Austria)",
    "title" : "A meta-learning approach to (re)discover plasticity rules that carve a desired function into a neural network",
    "url" : "https://vogelslab.org/",
    "image_src": "assets/img/tim_vogels.jpeg",
    "field": "Neuroscience",
    "date": "15.12.2021",
    "day": "Wednesday",
    "time": "1340",
     "abstract": "The search for biologically faithful synaptic plasticity rules has resulted in a large body of models. They are usually inspired by – and fitted to – experimental data, but they rarely produce neural dynamics that serve complex functions. These failures suggest that current plasticity models are still under-constrained by existing data. Here, we present an alternative approach that uses meta-learning to discover plausible synaptic plasticity rules. Instead of experimental data, the rules are constrained by the functions they implement and the structure they are meant to produce. Briefly, we parameterize synaptic plasticity rules by a Volterra expansion and then use supervised learning methods (gradient descent or evolutionary strategies) to minimize a problem-dependent loss function that quantifies how effectively a candidate plasticity rule transforms an initially random network into one with the desired function. We first validate our approach by re-discovering previously described plasticity rules, starting at the single-neuron level and “Oja’s rule”, a simple Hebbian plasticity rule that captures the direction of most variability of inputs to a neuron (i.e., the first principal component). We expand the problem to the network level and ask the framework to find Oja’s rule together with an anti-Hebbian rule such that an initially random two-layer firing-rate network will recover several principal components of the input space after learning. Next, we move to networks of integrate-and-fire neurons with plastic inhibitory afferents. We train for rules that achieve a target firing rate by countering tuned excitation. Our algorithm discovers a specific subset of the manifold of rules that can solve this task. Our work is a proof of principle of an automated and unbiased approach to unveil synaptic plasticity rules that obey biological constraints and can solve complex functions."
},
{
    "name" : "Rishikesh Narayanan",
    "last_name": "Narayan",
    "affiliation": "Indian Institute of Sciences, Bangalore, India",
    "title" : "Plasticity and computing in the brain: Oversimplifications",
    "url" : "http://mbu.iisc.ernet.in/~rngrp/",
    "image_src": "assets/img/Rishi.jpeg",
    "field": "Neuroscience",
    "date": "15.12.2021",
    "day": "Wednesday",
    "time": "1420",
     "abstract": "In this talk, the following (subset of) oversimplifications in assessing plasticity and computing in the brain will be discussed. Some approaches over the decades to account for these oversimplifications will be presented. Oversimplification #1. Neurons are simple algebraic summation units with a threshold nonlinearity. Oversimplification #2. Neural circuits are made of repeating homogeneous computational units. Oversimplification #3. Learning and memory in biological systems is accomplished exclusively through synaptic changes. Oversimplification #4. There is a unique solution to how biological learning is accomplished, and our goal is to find that solution. Oversimplification #5. Glial cells are glue. Other oversimplifications. Ignore the following across all scales of analysis: (i) Feedback loops (ii) Stochasticity and noise (iii) Continual adaptation (iv) Pleiotropy (v) Secondary and off-target effects of perturbations (vi) Subject-to-subject variability (vii) Plasticity-Stability balance (viii) Catastrophic forgetting."
},
{
    "name" : "Anita Disney",
    "last_name": "Disney",
    "affiliation": "Duke University, US",
    "title" : "Specifying a neural network: Lessons from neuromodulation in visual processing",
    "url" : "https://www.neuro.duke.edu/people/faculty/anita-disney",
    "image_src": "assets/img/Anita_Disney.jpeg",
    "field": "Neuroscience",
    "date": "15.12.2021",
    "day": "Wednesday",
    "time": "1600",
     "abstract": ""
},
{
    "name" : "Nelson Totah",
    "last_name": "Totah",
    "affiliation": " Helsinki Institute of Life Science HiLIFE",
    "title" : "TBA",
    "url" : "https://researchportal.helsinki.fi/en/persons/nelson-totah",
    "image_src": "assets/img/nelson_totah.jpeg",
    "field": "Neuroscience",
    "date": "15.12.2021",
    "day": "Wednesday",
    "time": "1640",
     "abstract": "Evolution has conferred upon natural neural circuits the capacity to specify their own processing states, on multiple spatial and temporal scales, in a manner that is adaptive to internal and external context. In many cases this state specification arises from the collective action of multiple chemical neuromodulators. Using the visual system of the macaque as a model, I will outline some perhaps lesser-known structural and functional features of the ways that these neuromodulatory state specification signals interact with neural circuits."
},
{
    "name" : "Gabrielle Gutierrez",
    "last_name": "Gutierrez",
    "affiliation": "Barnard College, Columbia University, NY, US",
    "title" : "The smart image compression algorithm in the retina: a theoretical study of recoding inputs in neural circuits",
    "url" : "https://gabriellejgutierrez.com/",
    "image_src": "assets/img/gabrielle.jpeg",
    "field": "Neuroscience",
    "date": "15.12.2021",
    "day": "Wednesday",
    "time": "1720",
     "abstract": "Computation in neural circuits relies on a common set of motifs, including divergence of common inputs to parallel pathways, convergence of multiple inputs to a single neuron, and nonlinearities that select some signals over others.  Convergence and circuit nonlinearities, considered individually, can lead to a loss of information about the inputs. Past work has detailed how to optimize nonlinearities and circuit weights to maximize information, but we show that selective nonlinearities, acting together with divergent and convergent circuit structure, can improve information transmission over a purely linear circuit despite the suboptimality of these components individually. These nonlinearities recode the inputs in a manner that preserves the variance among converged inputs. Our results suggest that neural circuits may be doing better than expected without finely tuned weights."
}
]
